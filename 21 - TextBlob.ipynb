{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "- In a lot of the cases in which we have been dealing with strings we don't take into account specifics about the contents, besides the literal word/spelling.\n",
    "- Using Natural Language Processing (or NLP) we are able to actually analyze specifics about the words themselves in the Strings. \n",
    "- We do this by using TextBlobs and the NLTK libraries. \n",
    "\n",
    "#### API for TextBlob https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.blob.WordList\n",
    "\n",
    "\n",
    "#### TextBlobs are another Python module/library that is prebuilt that we can use to do different things. \n",
    "#### They can do a lot of things, including \n",
    "- Tokenization—splitting text into pieces called tokens, which are meaningful units, such as words and numbers.\n",
    "- Parts-of-speech (POS) tagging—identifying each word’s part of speech, such as noun, verb, adjective, etc.\n",
    "- Noun phrase extraction—locating groups of words that represent nouns, such as “red brick factory.”\n",
    "- Sentiment analysis—determining whether text has positive, neutral or negative sentiment. \n",
    "- Inter-language translation and language detection powered by Google Translate.\n",
    "- Inflection—pluralizing and singularizing words. There are other aspects of inflection that are not part of TextBlob. \n",
    "- Word frequencies—determining how often each word appears in a corpus.\n",
    "- WordNet integration for finding word definitions, synonyms and antonyms.\n",
    "- Stop word elimination—removing common words, such as a, an, the, I, we, you and more to analyze the important words in a corpus.\n",
    "\n",
    "\n",
    "#### All of these are things that depending on the situation are very useful. Each of you has done some of these yourself in the previous labs.\n",
    "\n",
    "To download what we need we go into the anaconda commandline, in much of the same way we did earlier in the semester. We run the following commands and follow the instructions for each.\n",
    "\n",
    "- conda install -c conda-forge textblob\n",
    "- ipython -m textblob.download_corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we need to run the following code. The commented code should only be run once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\alanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\alanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\alanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%conda install pandas\n",
    "#%conda install numpy\n",
    "#%conda install textblob\n",
    "#%conda install requests\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets look at how these things actually work, by looking at some basic examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence, as it contains a subject and a verb. In fact it is a complex sentence! I think.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextBlob(\"This is a sentence, as it contains a subject and a verb. In fact it is a complex sentence! I think.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "string=\"This is a sentence, as it contains a subject and a verb. In fact it is a complex sentence! I think.\"\n",
    "\n",
    "blob=TextBlob(string)\n",
    "print(blob)\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The variable blob in the cell above, is a TextBlob object. This is important to recognize as it allows us to do more things with it than a traditional string. Note that when we just print out the TextBlob we just print what is contained in it, and frankly is not obvious that it came a TextBlob to begin with. Let's see what else it can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"This is a sentence, as it contains a subject and a verb.\"), Sentence(\"In fact it is a complex sentence!\"), Sentence(\"I think.\")]\n",
      "<class 'textblob.blob.Sentence'>\n",
      "This is a sentence, as it contains a subject and a verb.\n",
      "['This', 'is', 'a', 'sentence', 'as', 'it', 'contains', 'a', 'subject', 'and', 'a', 'verb', 'In', 'fact', 'it', 'is', 'a', 'complex', 'sentence', 'I', 'think']\n",
      "<class 'textblob.blob.Word'>\n",
      "This\n"
     ]
    }
   ],
   "source": [
    "# What if we wanted to break up via sentences? Like we did on a previous lab?\n",
    "sentences=blob.sentences\n",
    "\n",
    "# Note the .sentences does not have parentheses after it. It is known as a property and it's a pretty common technique in different libraries.\n",
    "\n",
    "print(sentences)\n",
    "\n",
    "# The type of each item in this list is a Sentence Object meaning it also has a slew of functions we can use on them.\n",
    "\n",
    "print(type(sentences[0]))\n",
    "print(sentences[0])\n",
    "\n",
    "\n",
    "\n",
    "# What if we wanted to break up via words?\n",
    "# It works just as you would expect, similar to the sentences above.\n",
    "\n",
    "words=blob.words\n",
    "\n",
    "print(words)\n",
    "print(type(words[0]))\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('as', 'IN'), ('it', 'PRP'), ('contains', 'VBZ'), ('a', 'DT'), ('subject', 'NN'), ('and', 'CC'), ('a', 'DT'), ('verb', 'NN'), ('In', 'IN'), ('fact', 'NN'), ('it', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('complex', 'JJ'), ('sentence', 'NN'), ('I', 'PRP'), ('think', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "## What if I was interested in the parts of speech of our particular words?\n",
    "partsOfSpeech=blob.tags\n",
    "print(partsOfSpeech)\n",
    "\n",
    "# This can be useful for us/computer to get a better understanding of what words\n",
    "# we are talking about as words can have huge amounts of meanings, and specifying\n",
    "# the POS (part of speech) can limit our possible definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ok but what do those tags mean? Lets go look it up!\n",
    "\n",
    "https://www.geeksforgeeks.org/python-part-of-speech-tagging-using-textblob/\n",
    "\n",
    "There is also a dictionary you can access below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of parts-of-speech markers.\n",
    "\n",
    "POSdict = {'CC': 'coordinating conjunction',\n",
    "'CD':'cardinal digit',\n",
    "'DT': 'determiner',\n",
    "'EX': \"existential there (like: 'there is' … think of it like 'there exists')\",\n",
    "'FW': 'foreign word',\n",
    "'IN': 'preposition/subordinating conjunction',\n",
    "'JJ': \"adjective 'big'\",\n",
    "'JJR': \"adjective, comparative 'bigger'\",\n",
    "'JJS': \"adjective, superlative 'biggest'\",\n",
    "'LS': 'list marker 1)',\n",
    "'MD': 'modal could, will',\n",
    "'NN': \"noun, singular 'desk'\",\n",
    "'NNS': \"noun plural 'desks'\",\n",
    "'NNP': \"proper noun, singular 'Harrison'\",\n",
    "'NNPS': \"proper noun, plural 'Americans'\",\n",
    "'PDT': \"predeterminer 'all the kids'\",\n",
    "'POS': \"possessive ending parent's\",\n",
    "'PRP':'personal pronoun I, he, she',\n",
    "'PRP$': 'possessive pronoun my, his, hers',\n",
    "'RB': 'adverb very, silently',\n",
    "'RBR': 'adverb, comparative better',\n",
    "'RBS': 'adverb, superlative best',\n",
    "'RP': 'particle give up',\n",
    "'TO': \"to go 'to' the store.\",\n",
    "'UH': 'interjection errrrrrrrm',\n",
    "'VB': 'verb, base form take',\n",
    "'VBD': 'verb, past tense took',\n",
    "'VBG': 'verb, gerund/present participle taking',\n",
    "'VBN': 'verb, past participle taken',\n",
    "'VBP': 'verb, sing. present, non-3d take',\n",
    "'VBZ': 'verb, 3rd person sing. present takes',\n",
    "'WDT': 'wh-determiner which',\n",
    "'WP': 'wh-pronoun who, what',\n",
    "'WP$': 'possessive wh-pronoun whose',\n",
    "'WRB' :'wh-adverb where, when'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'this': 1, 'is': 2, 'a': 4, 'sentence': 2, 'as': 1, 'it': 2, 'contains': 1, 'subject': 1, 'and': 1, 'verb': 1, 'in': 1, 'fact': 1, 'complex': 1, 'i': 1, 'think': 1})\n",
      "{'this': 1, 'is': 2, 'a': 4, 'sentence': 2, 'as': 1, 'it': 2, 'contains': 1, 'subject': 1, 'and': 1, 'verb': 1, 'in': 1, 'fact': 1, 'complex': 1, 'i': 1, 'think': 1}\n"
     ]
    }
   ],
   "source": [
    "# What if I wanted to get frequency like we did on previous labs?\n",
    "freqDict=blob.word_counts\n",
    "print(freqDict)\n",
    "\n",
    "#Note this returns a defaultdict object- if we want to convert it to a primitive python dict we can just convert literally\n",
    "print(dict(freqDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun Phrases\n",
    "Sometimes words individually don't hold as much meaning as they do in groups. For instance the word water ski- water and ski have two different meanings that only when put together make up the word water ski. Computers should be able to recognize that- thus we have noun phrases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love using my jet ski on a wonderful day like today.\n",
      "<class 'textblob.blob.WordList'>\n",
      "['jet ski', 'wonderful day']\n",
      "<class 'textblob.blob.Word'>\n",
      "jet ski\n"
     ]
    }
   ],
   "source": [
    "nounPhraseStr=TextBlob(\"I love using my jet ski on a wonderful day like today.\")\n",
    "print(nounPhraseStr)\n",
    "nounPhrase=nounPhraseStr.noun_phrases\n",
    "# Note: this is a wordList object, which acts like a list but allows for extra functions to be called on them\n",
    "print(type(nounPhrase))\n",
    "print(nounPhrase)\n",
    "\n",
    "#Additionally even the multiple words in each element of the wordList is still a word object\n",
    "print(type(nounPhrase[0]))\n",
    "print(nounPhrase[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets take a look at the basics of sentiment analysis.\n",
    "Sentiment Analysis is the idea of taking some sentence or series of sentences and tring to quantatatively discuss whether the sentence is positive, negative, or neutral.\n",
    "\n",
    "TextBlobs do this by assigning a polarity value from -1 to 1. The closer to negative 1 the more negative the sentiment, the closer to positive 1 the more positive, and the closest to 0 the more neutral.\n",
    "\n",
    "TextBlobs also can rate subjectivity vs objectivity. Thus TextBlobs gives a subjectivity rating from 0 to 1. The higher the value the more subjective the sentence is. Lower values come from a more objective statment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That food was terrible! Sentiment(polarity=-1.0, subjectivity=1.0)\n",
      "The food was mediocre at best. Sentiment(polarity=0.25, subjectivity=0.65)\n",
      "The food was excellent. Sentiment(polarity=1.0, subjectivity=1.0)\n",
      "My name is William. Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "-1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "angrySent=TextBlob(\"That food was terrible!\")\n",
    "\n",
    "neutralStr=TextBlob(\"The food was mediocre at best.\")\n",
    "\n",
    "positiveStr=TextBlob(\"The food was excellent.\")\n",
    "\n",
    "objectiveStr=TextBlob(\"My name is William.\")\n",
    "\n",
    "print(angrySent,angrySent.sentiment)\n",
    "\n",
    "print(neutralStr,neutralStr.sentiment)\n",
    "\n",
    "print(positiveStr,positiveStr.sentiment)\n",
    "\n",
    "print(objectiveStr,objectiveStr.sentiment)\n",
    "\n",
    "#Note on the objective sentence we have a polarity of zero and a subjectivity of zero, meaning we have a neutral sentence with absolute objectivity.\n",
    "\n",
    "\n",
    "#If we wanted to get the specfic values for polarity or subjectivity out of this subjectivity object, we could do the following-\n",
    "print(angrySent.sentiment.polarity)\n",
    "print(angrySent.sentiment[0])\n",
    "print(angrySent.sentiment.subjectivity)\n",
    "print(angrySent.sentiment[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TextBlob library also comes with a NaiveBayesAnalyzer, which was trained on a database of movie reviews. Naive Bayesis acommonly used machine learning text-classification algorithm. The following uses the analyzer keyword argument to specify a TextBlob’s sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That food was terrible! Sentiment(classification='neg', p_pos=0.22647144513750855, p_neg=0.7735285548624911)\n"
     ]
    }
   ],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "angrySent=TextBlob(\"That food was terrible!\",analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "print(angrySent,angrySent.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word objects allow us to do alot more things then just look at sentences as a whole. What if we were interested in getting definitions? Or synonyms? Antonyms? All of these are possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "word=Word(\"Happy\")\n",
    "\n",
    "#print(word.definitions)\n",
    "#print(word.synsets)\n",
    "\n",
    "###Each Synset represents a group of synonyms. In the notation happy.a.01:\n",
    "#• happy is the original Word’s lemmatized form (in this case, it’s the same).\n",
    "#• a is the part of speech, which can be a for adjective, n for noun, v for verb, r for\n",
    "#adverb or s for adjective satellite. Many adjective synsets in WordNet have satel\u0002lite synsets that represent similar adjectives.\n",
    "#• 01 is a 0-based index number. Many words have multiple meanings, and this is\n",
    "#the index number of the corresponding meaning in the WordNet database.\n",
    "\n",
    "#Note you could pass a POS into a different function .get_synsets() to only get a specifc POS in the set\n",
    "#word=Word(\"House\")\n",
    "#synset=word.get_synsets('n')\n",
    "#print(word,synset)\n",
    "#synset=word.get_synsets('v')\n",
    "#print(word,synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might remember us talking about stop words on the sonnet lab. Those came from NLTK! It also includes a lot more natural language stopwords. Let's focus on the english ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'josh', '32', 'teach', 'math', 'computer', 'science']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "#print((stops))\n",
    "\n",
    "##Note for below! We can still do a lot of our string manipulation tricks, lower upper, slicing etc all can be done on TextBlobs\n",
    "englishString=TextBlob(\"My name is Josh. I am 32. I teach math and computer science.\").lower()\n",
    "## We can use this list of stopwords to eliminate them just like we did before. \n",
    "wordlst=[word for word in englishString.words if word not in stops]\n",
    "print(wordlst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our hand at a bit of analyis similar to what we did before. Lets make some plots on the frequency of non stopwords in Romeo and Juliet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'romeoJuliet.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_498/2793693837.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Open a file- read it into a string and turn it directly into a TextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"romeoJuliet.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdataString\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'romeoJuliet.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Open a file- read it into a string and turn it directly into a TextBlob\n",
    "file=open(\"romeoJuliet.txt\")\n",
    "dataString=TextBlob(file.read().lower())\n",
    "file.close()\n",
    "\n",
    "dataString=dataString.replace(\"â€™\",'')\n",
    "print(type(dataString))\n",
    "\n",
    "\n",
    "freqDict=dict(dataString.word_counts).items()\n",
    "\n",
    "words = [word for word in freqDict if word[0] not in stops]\n",
    "\n",
    "#print(words)\n",
    "\n",
    "\n",
    "def getItem(t):\n",
    "    return t[1]\n",
    "\n",
    "\n",
    "\n",
    "sortedFreqListKeys=sorted(words, key=getItem,reverse=True)[:20]\n",
    "print(sortedFreqListKeys)\n",
    "\n",
    "df=pd.DataFrame(sortedFreqListKeys,columns=[\"Word\",\"Frequency\"])\n",
    "axes=df.plot.bar(x='Word', y='Frequency', legend=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again turn to the text of the Constitution found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://www.usconstitution.net/const.txt')\n",
    "text = response.text\n",
    "\n",
    "text = text[278:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Turn this string into a TextBlob. Create a list of all the sentences in the Constitution and a list of all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the functions we have discussed today, make a dictionary of the word frequencies in the Constitution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine the parts of speech for each word in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a program which counts the different parts of speech. Display that information in a table (i.e. a Pandas DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Perform a sentiment analysis on each sentence of the text and store that information as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a histogram of all the positive sentiments. (Hint: `sentence.sentiment[1]` is the positive sentiment for `sentence`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
